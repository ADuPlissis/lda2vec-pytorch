{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load doc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = pd.read_hdf('data.hdf', 'data')\n",
    "docs = list(D['docs'])\n",
    "docs = [d.split(' ') for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13812"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for d in docs:\n",
    "    tokens += d\n",
    "\n",
    "term_counts = Counter(tokens)\n",
    "\n",
    "# number of unique words\n",
    "len(term_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (term, term_count)\n",
    "term_counts = [(k, term_counts[k]) for k in term_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ax', 62552),\n",
       " ('know', 7385),\n",
       " ('good', 6652),\n",
       " ('like', 6573),\n",
       " ('people', 6473),\n",
       " ('don', 6457),\n",
       " ('think', 6206),\n",
       " ('time', 5747),\n",
       " ('use', 5531),\n",
       " ('max', 4747)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by counts \n",
    "term_counts = sorted(term_counts, key=lambda x: x[1], reverse=True)\n",
    "term_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrify', 9),\n",
       " ('dividians', 8),\n",
       " ('brag', 8),\n",
       " ('winmarks', 8),\n",
       " ('penio', 8),\n",
       " ('ahhh', 8),\n",
       " ('harvest', 8),\n",
       " ('ithaca', 8),\n",
       " ('freewill', 8),\n",
       " ('encrypting', 7)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_counts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582513"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of counts\n",
    "counts_sum = sum([t[1] for t in term_counts])\n",
    "counts_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_dist = {i: v[1]/counts_sum for i, v in enumerate(term_counts, 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode words with integers\n",
    "encode = {v[0]: i for i, v in enumerate(term_counts, 0)}\n",
    "decode = {i: v[0] for i, v in enumerate(term_counts, 0)}\n",
    "# 0 - the most common word, 1 - the second most common word, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_docs = D['docs'].apply(lambda x: x.split(' ')).apply(lambda x: [encode[w] for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [84, 2685, 542, 288, 3019, 791, 180, 39, 822, ...\n",
       "1    [1206, 675, 70, 603, 344, 98, 53, 2157, 215, 4...\n",
       "2    [754, 10, 1716, 10509, 13, 202, 97, 12, 3, 244...\n",
       "3    [6, 253, 98, 2687, 757, 140, 253, 98, 2687, 75...\n",
       "4    [104, 36, 8, 13, 16, 1281, 181, 1040, 108, 127...\n",
       "Name: docs, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hws = 5 # half window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc: a list of words\n",
    "def get_windows(doc, hws):\n",
    "    # it returns a list of tuples,\n",
    "    # each tuple looks like this:\n",
    "    # (word w, [hws words that come before w] + [hws words that come after w])\n",
    "    \n",
    "    length = len(doc)\n",
    "    \n",
    "    if length > 2*hws:\n",
    "        \n",
    "        inside = [(w, doc[(i - hws):i] + doc[(i + 1):(i + hws + 1)]) \n",
    "                  for i, w in enumerate(doc[hws:-hws], hws)]\n",
    "        \n",
    "        # For words that are near the beginning or\n",
    "        # the end of doc tuples are slightly different\n",
    "        \n",
    "        beginning = [(w, doc[:i] + doc[(i + 1):(2*hws + 1)]) \n",
    "                     for i, w in enumerate(doc[:hws], 0)]\n",
    "        \n",
    "        end = [(w, doc[-(2*hws + 1):i] + doc[(i + 1):]) \n",
    "               for i, w in enumerate(doc[-hws:], length - hws)]\n",
    "        \n",
    "        return beginning + inside + end\n",
    "    else:\n",
    "        print('Error! Not enough words in doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "windows = encoded_docs.apply(lambda x: get_windows(x, hws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for lda2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16116it [00:02, 6612.25it/s] \n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for index, list_of_windows in tqdm(windows.iteritems()):\n",
    "    # index represents id of a document, \n",
    "    # list_of_windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    data += [[index, w[0]] + w[1] for w in list_of_windows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a row in the data contains:\n",
    "# id of a document, id of a word in this document, a window around this word\n",
    "# 1 + 1 + 10\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582513"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of windows\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_vector(word):\n",
    "    return nlp.vocab[word].has_vector\n",
    "\n",
    "def get_vector(word):\n",
    "    return nlp.vocab[word].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(term_counts)\n",
    "embedding_dim = 300\n",
    "\n",
    "word_vectors = np.zeros((vocab_size, embedding_dim), 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_vec = [t for t in term_counts if not has_vector(t[0])]\n",
    "with_vec = [t for t in term_counts if has_vector(t[0])]\n",
    "len(no_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def approximately_equal(x, y):\n",
    "    return abs(x - y) < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initial_word_vector(word_count):\n",
    "    close_words = [t[0] for t in with_vec if approximately_equal(t[1], word_count)]\n",
    "    if len(close_words) == 0:\n",
    "        print('no words with similar count')\n",
    "    return np.array([get_vector(w) for w in close_words]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13812it [00:27, 504.57it/s] \n"
     ]
    }
   ],
   "source": [
    "for i, t in tqdm(enumerate(term_counts)):\n",
    "    if t in with_vec:\n",
    "        word_vectors[i] = get_vector(t[0])\n",
    "    else:\n",
    "        word_vectors[i] = get_initial_word_vector(t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('window_data.npy', data) # ~145 MB\n",
    "np.save('word_vectors.npy', word_vectors) # ~16 MB\n",
    "np.save('unigram_distribution.npy', unigram_dist)\n",
    "np.save('decode.npy', decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
