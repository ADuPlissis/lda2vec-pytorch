{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix bug: https://github.com/explosion/spaCy/issues/922\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.en.word_sets.STOP_WORDS, spacy.attrs.IS_STOP);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "docs = dataset['data']\n",
    "targets = dataset['target']\n",
    "target_names = dataset['target_names']\n",
    "\n",
    "# number of documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  3 17 ...,  3  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clean documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spaces = re.compile(r' +')\n",
    "nonletters = re.compile(r'[^a-z A-Z]+')\n",
    "shortwords = re.compile(r'\\b\\w\\b')\n",
    "\n",
    "def clean(doc):\n",
    "    text = doc\n",
    "    text = re.sub(nonletters, ' ', text)\n",
    "    text = re.sub(shortwords, ' ', text)\n",
    "    text = re.sub(spaces, ' ', text).strip()\n",
    "    text = nlp(text, tag=True, parse=False, entity=False)\n",
    "    return [t.lemma_ for t in text if not t.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:34<00:00, 547.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85608"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    words = clean(doc)\n",
    "    preprocessed_docs += [words]\n",
    "    tokens += words\n",
    "\n",
    "term_counts = Counter(tokens)\n",
    "\n",
    "# number of unique tokens\n",
    "len(term_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13812"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_rare = [x for x in term_counts if term_counts[x] >= 10]\n",
    "len(non_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [02:49<00:00, 111.23it/s]\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for doc in tqdm(preprocessed_docs):\n",
    "    temp.append([w for w in doc if w in non_rare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16116"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep documents with more than 10 words\n",
    "to_keep = [i for i, doc in enumerate(temp) if len(doc) > 10]\n",
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_docs = [temp[i] for i in to_keep]\n",
    "targets = targets[to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some preprocessed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'pens', 'fan', 'pretty', 'confused', 'lack', 'kind', 'post', 'recent', 'pens', 'massacre', 'devil', 'actually', 'bit', 'bit', 'go', 'end', 'non', 'relief', 'bit', 'praise', 'pens', 'man', 'kill', 'devil', 'bad', 'think', 'jagr', 'show', 'good', 'regular', 'season', 'stat', 'lot', 'fo', 'fun', 'watch', 'playoff', 'bowman', 'let', 'jagr', 'lot', 'fun', 'couple', 'game', 'pens', 'go', 'beat', 'jersey', 'disappointed', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brother', 'market', 'high', 'performance', 'video', 'card', 'support', 'vesa', 'local', 'bus', 'mb', 'ram', 'suggestion', 'idea', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'ati', 'graphics', 'ultra', 'pro', 'high', 'performance', 'vlb', 'card', 'post', 'email', 'thank', 'matt']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_as_strings = [' '.join(doc) for doc in preprocessed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = pd.DataFrame({'docs': docs_as_strings, 'targets': targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert targets to human readable format\n",
    "target_dict = {i: n for i, n in enumerate(target_names)}\n",
    "D['targets'] = D['targets'].apply(lambda x: target_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D.to_hdf('data.hdf', 'data', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
