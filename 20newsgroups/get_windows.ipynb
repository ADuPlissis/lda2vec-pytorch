{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import preprocess, get_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNTS = 20\n",
    "MAX_COUNTS = 1800\n",
    "# words with count < MIN_COUNTS\n",
    "# and count > MAX_COUNTS\n",
    "# will be removed\n",
    "\n",
    "MIN_LENGTH = 15\n",
    "# minimum document length \n",
    "# (number of words)\n",
    "# after preprocessing\n",
    "\n",
    "HALF_WINDOW_SIZE = 5\n",
    "# it must be that 2*HALF_WINDOW_SIZE < MIN_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "docs = dataset['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store an index with a doc\n",
    "docs = [(i, doc) for i, doc in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataset and create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:48<00:00, 391.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed short documents: 3985\n",
      "total number of tokens: 1439861\n",
      "number of tokens to be removed: 393091\n",
      "number of additionally removed short documents: 2032\n",
      "total number of tokens: 1023189\n",
      "\n",
      "minimum word count number: 14\n",
      "this number can be less than MIN_COUNTS because of document removal\n"
     ]
    }
   ],
   "source": [
    "encoded_docs, decoder, word_counts = preprocess(\n",
    "    docs, nlp, MIN_LENGTH, MIN_COUNTS, MAX_COUNTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new ids will be created for the documents.\n",
    "# create a way of restoring initial ids:\n",
    "doc_decoder = {i: doc_id for i, (doc_id, doc) in enumerate(encoded_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12829it [00:03, 4216.57it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for index, (_, doc) in tqdm(enumerate(encoded_docs)):\n",
    "    windows = get_windows(doc, HALF_WINDOW_SIZE)\n",
    "    # index represents id of a document, \n",
    "    # windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    data += [[index, w[0]] + w[1] for w in windows]\n",
    "\n",
    "data = np.array(data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a row in 'data' contains:\n",
    "# id of a document, id of a word in this document, a window around this word\n",
    "# 1 + 1 + 10\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023189"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of windows (equals to the total number of tokens)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = np.array(word_counts)\n",
    "unigram_distribution = word_counts/sum(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7460"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(decoder)\n",
    "embedding_dim = 64\n",
    "\n",
    "word_vectors = np.random.normal(\n",
    "    0.0, np.sqrt(1.0/embedding_dim), \n",
    "    size=(vocab_size, embedding_dim)\n",
    ").astype('float32')\n",
    "\n",
    "# number of unique words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare initialization for document weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 1.18 s, total: 1min 23s\n",
      "Wall time: 45.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = 20\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=n_topics)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : myer player hit ground ball wire john fan radar play\n",
      "topic 1 : car buy sell price bike gun sale pay engine lot\n",
      "topic 2 : book stephanopoulos president church talk job vote consider revelation statement\n",
      "topic 3 : team play win player hockey season gun firearm league nhl\n",
      "topic 4 : server widget window copy client application sun function display library\n",
      "topic 5 : bike father apartment woman daughter old motorcycle door iran live\n",
      "topic 6 : homosexual sex homosexuality male sexual paul giz marriage sin disease\n",
      "topic 7 : space turkey dro stanley disk voyager alarm commercial launcher quantum\n",
      "topic 8 : space launch earth orbit nasa mission satellite moon solar planet\n",
      "topic 9 : church public issue president school house general federal today attorney\n",
      "topic 10 : medical patient doctor treatment cause disease pain cancer test medicine\n",
      "topic 11 : jesus word san israel lose creator west yassin mormon text\n",
      "topic 12 : child kill fire happen fbi leave home war woman away\n",
      "topic 13 : claim belief religion life christian jesus word bible israel reason\n",
      "topic 14 : bit des message ripem heat chip block light random sound\n",
      "topic 15 : ripem article newsgroup posting request address faq section message science\n",
      "topic 16 : software user version window datum color format application code display\n",
      "topic 17 : armenian turkish lord armenians turkey history genocide university world turks\n",
      "topic 18 : card disk driver board monitor mac memory windows machine hard\n",
      "topic 19 : drug health money increase report cost rate tax pay crime\n"
     ]
    }
   ],
   "source": [
    "for i, topics in lda.show_topics(n_topics, formatted=False):\n",
    "    print('topic', i, ':', ' '.join([t for t, _ in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12829/12829 [00:28<00:00, 456.76it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_weights_init = np.zeros((len(corpus_lda), n_topics))\n",
    "for i in tqdm(range(len(corpus_lda))):\n",
    "    topics = corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = doc_weights_init + np.random.uniform(high=0.1, size=doc_weights_init.shape)\n",
    "doc_weights_init = np.log(x/x.sum(1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data.npy', data)\n",
    "np.save('word_vectors.npy', word_vectors)\n",
    "np.save('unigram_distribution.npy', unigram_distribution)\n",
    "np.save('decoder.npy', decoder)\n",
    "np.save('doc_decoder.npy', doc_decoder)\n",
    "np.save('doc_weights_init.npy', doc_weights_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
